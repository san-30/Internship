{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfbb590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sank\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sank\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=eec62648ea39e5e2656db7630a5b50ded916bfd497c467466547e4fa3717d47d\n",
      "  Stored in directory: c:\\users\\sank\\appdata\\local\\pip\\cache\\wheels\\73\\2b\\cb\\099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "Requirement already satisfied: requests in c:\\users\\sank\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sank\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sank\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sank\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sank\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64da28cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35fc29cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Status_code:  200\n",
      "                     Header Tags\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "# 1. Write a python prgram to display all the header tags from wikipedia.org and make dataframe.\n",
    "\n",
    "def wikipedia_all_headers_display(URL):\n",
    "    page = requests.get(URL)\n",
    "    print(\"Page Status_code: \", page.status_code)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "    header_tag = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    \n",
    "    header = [i.text.strip() for i in header_tag]\n",
    "    return header\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "header_tag = wikipedia_all_headers_display(URL)\n",
    "df = pd.DataFrame({'Header Tags':header_tag})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c907d3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Status_code:  200\n",
      "          Name of the presidents          Presidents Term\n",
      "0           Shri Ram Nath Kovind  14th President of India\n",
      "1          Shri Pranab Mukherjee  13th President of India\n",
      "2   Smt Pratibha Devisingh Patil  12th President of India\n",
      "3         DR. A.P.J. Abdul Kalam  11th President of India\n",
      "4           Shri K. R. Narayanan  10th President of India\n",
      "5        Dr Shankar Dayal Sharma  9th  President of India\n",
      "6            Shri R Venkataraman   8th President of India\n",
      "7               Giani Zail Singh   7th President of India\n",
      "8      Shri Neelam Sanjiva Reddy   6th President of India\n",
      "9       Dr. Fakhruddin Ali Ahmed   5th President of India\n",
      "10  Shri Varahagiri Venkata Giri   4th President of India\n",
      "11              Dr. Zakir Husain   3rd President of India\n",
      "12  Dr. Sarvepalli Radhakrishnan   2nd President of India\n",
      "13           Dr. Rajendra Prasad   1st President of India\n"
     ]
    }
   ],
   "source": [
    "# 2. Write a python program to display the list of the respected former presidents of India (i.e. Name, Term ofoffice) \n",
    "# from https://presidentofindia.nic.in/former-presidents and make DataFrame.\n",
    "\n",
    "def name_president(president_URL):\n",
    "    page = requests.get(president_URL)\n",
    "    print(\"Page Status_code: \", page.status_code)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "  \n",
    "    name_ele = soup.find_all(\"h3\")\n",
    "    name = [i.text.strip() for i in name_ele]\n",
    "\n",
    "    return name\n",
    "\n",
    "def term_president(president_URL):\n",
    "    page = requests.get(president_URL)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "  \n",
    "    term_ele = soup.find_all(\"h5\")\n",
    "    term = [i.text.strip() for i in term_ele]\n",
    "\n",
    "    return term\n",
    "\n",
    "president_URL = \"https://presidentofindia.nic.in/former-presidents\"\n",
    "presidents_name = name_president(president_URL)\n",
    "presidents_term = term_president(president_URL)\n",
    "df = pd.DataFrame({'Name of the presidents':presidents_name, \"Presidents Term\": presidents_term})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be088c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Status_code:  200\n",
      "------------------------------------------- \n",
      "\n",
      "Top 10 ODI Teams \n",
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      27  3,112    115\n",
      "1     Pakistan\\nPAK      27  3,102    115\n",
      "2        India\\nIND      40  4,558    114\n",
      "3      England\\nENG      28  2,942    105\n",
      "4  South Africa\\nSA      23  2,386    104\n",
      "5   New Zealand\\nNZ      31  3,110    100\n",
      "6   Bangladesh\\nBAN      33  3,107     94\n",
      "7     Sri Lanka\\nSL      37  3,448     93\n",
      "8  Afghanistan\\nAFG      21  1,687     80\n",
      "9   West Indies\\nWI      38  2,582     68\n",
      "------------------------------------------- \n",
      "\n",
      "*************************************** \n",
      "\n",
      "Top 10 ODI Batsmen \n",
      "             Player_Name Team Rating\n",
      "0             Babar Azam  PAK    863\n",
      "1           Shubman Gill  IND    759\n",
      "2  Rassie van der Dussen   SA    745\n",
      "3           David Warner  AUS    739\n",
      "4            Imam-ul-Haq  PAK    735\n",
      "5           Harry Tector  IRE    726\n",
      "6        Quinton de Kock   SA    721\n",
      "7            Virat Kohli  IND    715\n",
      "8           Rohit Sharma  IND    707\n",
      "9           Fakhar Zaman  PAK    705\n",
      "*************************************** \n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++ \n",
      "\n",
      "Top 10 ODI Bowlers \n",
      "        Player_Name Team Rating\n",
      "0    Josh Hazlewood  AUS    692\n",
      "1    Mitchell Starc  AUS    666\n",
      "2       Trent Boult   NZ    666\n",
      "3        Adam Zampa  AUS    663\n",
      "4        Matt Henry   NZ    658\n",
      "5  Mujeeb Ur Rahman  AFG    657\n",
      "6     Kuldeep Yadav  IND    656\n",
      "7       Rashid Khan  AFG    655\n",
      "8    Mohammed Siraj  IND    643\n",
      "9    Shaheen Afridi  PAK    635\n",
      "++++++++++++++++++++++++++++++++++++++++ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Write a python prgram to scarpe cricket rankings from https://www.icc-cricket.com/rankings/mens/player-rankings/test\n",
    "# you have to scrape and make dataframe\n",
    "# a)\tTop 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "# b)\tTop 10 ODI Batsmen along with the records of their team and rating.\n",
    "# c)\tTop 10 ODI bowlers along with the records of their team and rating.\n",
    "\n",
    "\n",
    "def top_10_ODI_teams_in_men_cricket(odi_URL1):\n",
    "    page = requests.get(odi_URL1)\n",
    "    print(\"Page Status_code: \", page.status_code)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "    odi = []\n",
    "    \n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "    rows = table.find_all('tr')[1:11]\n",
    "\n",
    "    for i in rows:\n",
    "        cols = i.find_all('td')\n",
    "        team_name = cols[1].text.strip()\n",
    "        matches = cols[2].text.strip()\n",
    "        points = cols[3].text.strip()\n",
    "        rating = cols[4].text.strip()\n",
    "        odi.append([team_name, matches, points, rating])\n",
    "    return odi\n",
    "\n",
    "def top_10_ODI_batsmen(batsmen_URL2):\n",
    "    page = requests.get(batsmen_URL2)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "    batsmen = []\n",
    "    \n",
    "    table = soup.find('table', {'class': 'table rankings-table'})\n",
    "    rows = table.find_all('tr')[1:11]\n",
    "\n",
    "    for i in rows:\n",
    "        cols = i.find_all('td')\n",
    "        player_name = cols[1].text.strip()\n",
    "        team = cols[2].text.strip()\n",
    "        rating = cols[3].text.strip()\n",
    "        batsmen.append([player_name, team, rating])\n",
    "        \n",
    "    return batsmen\n",
    "\n",
    "def top_10_ODI_bowlers(bowler_URL3):\n",
    "    page = requests.get(bowler_URL3)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "    bowlers = []\n",
    "    \n",
    "    table = soup.find('table', {'class': 'table rankings-table'})\n",
    "    rows = table.find_all('tr')[1:11]\n",
    "\n",
    "    for i in rows:\n",
    "        cols = i.find_all('td')\n",
    "        player_name = cols[1].text.strip()\n",
    "        team = cols[2].text.strip()\n",
    "        rating = cols[3].text.strip()\n",
    "        bowlers.append([player_name, team, rating])\n",
    "        \n",
    "    return bowlers\n",
    "\n",
    "odi_URL1 = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "odi_top_10_teams = top_10_ODI_teams_in_men_cricket(odi_URL1)\n",
    "df_odi_top_10_teams = pd.DataFrame(odi_top_10_teams, columns=['Team', 'Matches', 'Points', 'Rating'])\n",
    "print(\"------------------------------------------- \\n\")\n",
    "print(\"Top 10 ODI Teams \")\n",
    "print(df_odi_top_10_teams)\n",
    "print(\"------------------------------------------- \\n\")\n",
    "\n",
    "batsmen_URL2 = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "odi_top_10_batsmen = top_10_ODI_batsmen(batsmen_URL2)\n",
    "df_odi_top_10_batsmen = pd.DataFrame(odi_top_10_batsmen, columns=['Player_Name', 'Team', 'Rating'])\n",
    "print(\"*************************************** \\n\")\n",
    "print(\"Top 10 ODI Batsmen \")\n",
    "print(df_odi_top_10_batsmen)\n",
    "print(\"*************************************** \\n\")\n",
    "\n",
    "bowler_URL3 = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "odi_top_10_bowlers = top_10_ODI_bowlers(bowler_URL3)\n",
    "df_odi_top_10_bowlers = pd.DataFrame(odi_top_10_bowlers, columns=['Player_Name', 'Team', 'Rating'])\n",
    "print(\"++++++++++++++++++++++++++++++++++++++++ \\n\")\n",
    "print(\"Top 10 ODI Bowlers \")\n",
    "print(df_odi_top_10_bowlers)\n",
    "print(\"++++++++++++++++++++++++++++++++++++++++ \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "898165f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Status_code:  200\n",
      "------------------------------------------- \n",
      "\n",
      "Top 10 ODI Teams \n",
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      26  4,290    165\n",
      "1      England\\nENG      31  3,875    125\n",
      "2  South Africa\\nSA      26  3,098    119\n",
      "3        India\\nIND      30  3,039    101\n",
      "4   New Zealand\\nNZ      28  2,688     96\n",
      "5   West Indies\\nWI      29  2,743     95\n",
      "6   Bangladesh\\nBAN      17  1,284     76\n",
      "7     Sri Lanka\\nSL      12    820     68\n",
      "8     Thailand\\nTHA      13    883     68\n",
      "9     Pakistan\\nPAK      27  1,678     62\n",
      "------------------------------------------- \n",
      "\n",
      "*************************************** \n",
      "\n",
      "Top 10 ODI Batswomen \n",
      "            Player_Name Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    801\n",
      "1           Beth Mooney  AUS    751\n",
      "2   Chamari Athapaththu   SL    743\n",
      "3       Laura Wolvaardt   SA    708\n",
      "4       Smriti Mandhana  IND    708\n",
      "5          Alyssa Healy  AUS    702\n",
      "6      Harmanpreet Kaur  IND    694\n",
      "7          Ellyse Perry  AUS    686\n",
      "8           Meg Lanning  AUS    682\n",
      "9       Stafanie Taylor   WI    618\n",
      "*************************************** \n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++ \n",
      "\n",
      "Top 10 ODI Bowlers \n",
      "           Player_Name Team Rating\n",
      "0    Sophie Ecclestone  ENG    761\n",
      "1       Shabnim Ismail   SA    708\n",
      "2        Jess Jonassen  AUS    682\n",
      "3     Ashleigh Gardner  AUS    673\n",
      "4         Megan Schutt  AUS    666\n",
      "5      Hayley Matthews   WI    662\n",
      "6           Kate Cross  ENG    660\n",
      "7       Ayabonga Khaka   SA    646\n",
      "8        Deepti Sharma  IND    607\n",
      "9  Rajeshwari Gayakwad  IND    599\n",
      "++++++++++++++++++++++++++++++++++++++++ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Write a python prgram to scarpe cricket rankings from https://www.icc-cricket.com/rankings/womens/player-rankings/odi\n",
    "# you have to scrape and make dataframe\n",
    "# a)\tTop 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "# b)\tTop 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "# c)\tTop 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "\n",
    "    \n",
    "def top_10_ODI_teams_in_women_cricket(odi_URL1):\n",
    "    page = requests.get(odi_URL1)\n",
    "    print(\"Page Status_code: \", page.status_code)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "    odi = []\n",
    "    \n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "    rows = table.find_all('tr')[1:11]\n",
    "\n",
    "    for i in rows:\n",
    "        cols = i.find_all('td')\n",
    "        team_name = cols[1].text.strip()\n",
    "        matches = cols[2].text.strip()\n",
    "        points = cols[3].text.strip()\n",
    "        rating = cols[4].text.strip()\n",
    "        odi.append([team_name, matches, points, rating])\n",
    "    return odi\n",
    "\n",
    "def top_10_ODI_batswomen(batsmen_URL2):\n",
    "    page = requests.get(batsmen_URL2)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "    batswomen = []\n",
    "    \n",
    "    table = soup.find('table', {'class': 'table rankings-table'})\n",
    "    rows = table.find_all('tr')[1:11]\n",
    "\n",
    "    for i in rows:\n",
    "        cols = i.find_all('td')\n",
    "        player_name = cols[1].text.strip()\n",
    "        team = cols[2].text.strip()\n",
    "        rating = cols[3].text.strip()\n",
    "        batswomen.append([player_name, team, rating])\n",
    "        \n",
    "    return batswomen\n",
    "\n",
    "def top_10_ODI_bowlers(bowler_URL3):\n",
    "    page = requests.get(bowler_URL3)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "    bowlers = []\n",
    "    \n",
    "    table = soup.find('table', {'class': 'table rankings-table'})\n",
    "    rows = table.find_all('tr')[1:11]\n",
    "\n",
    "    for i in rows:\n",
    "        cols = i.find_all('td')\n",
    "        player_name = cols[1].text.strip()\n",
    "        team = cols[2].text.strip()\n",
    "        rating = cols[3].text.strip()\n",
    "        bowlers.append([player_name, team, rating])\n",
    "        \n",
    "    return bowlers\n",
    "\n",
    "odi_URL1 = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "odi_top_10_teams = top_10_ODI_teams_in_women_cricket(odi_URL1)\n",
    "df_odi_top_10_teams = pd.DataFrame(odi_top_10_teams, columns=['Team', 'Matches', 'Points', 'Rating'])\n",
    "print(\"------------------------------------------- \\n\")\n",
    "print(\"Top 10 ODI Teams \")\n",
    "print(df_odi_top_10_teams)\n",
    "print(\"------------------------------------------- \\n\")\n",
    "\n",
    "batswomen_URL2 = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "odi_top_10_batswomen = top_10_ODI_batswomen(batswomen_URL2)\n",
    "df_odi_top_10_batswomen = pd.DataFrame(odi_top_10_batswomen, columns=['Player_Name', 'Team', 'Rating'])\n",
    "print(\"*************************************** \\n\")\n",
    "print(\"Top 10 ODI Batswomen \")\n",
    "print(df_odi_top_10_batswomen)\n",
    "print(\"*************************************** \\n\")\n",
    "\n",
    "bowler_URL3 = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling\"\n",
    "odi_top_10_bowlers = top_10_ODI_bowlers(bowler_URL3)\n",
    "df_odi_top_10_bowlers = pd.DataFrame(odi_top_10_bowlers, columns=['Player_Name', 'Team', 'Rating'])\n",
    "print(\"++++++++++++++++++++++++++++++++++++++++ \\n\")\n",
    "print(\"Top 10 ODI Bowlers \")\n",
    "print(df_odi_top_10_bowlers)\n",
    "print(\"++++++++++++++++++++++++++++++++++++++++ \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9d3307b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Status_code:  200\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++ \n",
      "\n",
      "Headline\n",
      "                                       cnbc_headlines\n",
      "0   Ray Dalio says to hold cash 'temporarily' — bu...\n",
      "1   The meme stock mania is now a movie. Here's wh...\n",
      "2   The Fed is expected to hold rates steady next ...\n",
      "3   Arm's second trading day is more subdued, but ...\n",
      "4   Enormous storm Lee lashes New England and Cana...\n",
      "5   Dow sheds nearly 300 points Friday, S&P 500 an...\n",
      "6   North Korea's Kim views Russian nuclear-capabl...\n",
      "7   Bumpy, hot and dusty: McLaren's Lando Norris o...\n",
      "8   UAW strike brings blue-collar vs. billionaire ...\n",
      "9   These bonds offer high yields for taking on hu...\n",
      "10  Poland, Hungary, Slovakia to introduce own ban...\n",
      "11  Planet Fitness shares sink after board ousts C...\n",
      "12  U.S. ambassador to Russia visits jailed report...\n",
      "13  Instacart aims for valuation of up to $10 bill...\n",
      "14  Shipping giant Maersk is seeing tentative sign...\n",
      "15  UK designates Wagner Group as terrorists; Russ...\n",
      "16  Taiwan slams Elon Musk, says it's 'not for sal...\n",
      "17  Oil just hit its highest level of the year — a...\n",
      "18  With outlaws for allies, is Russia becoming an...\n",
      "19  Shares of K-pop agency plunge 9% after Blackpi...\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++ \n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++ \n",
      "\n",
      "Time\n",
      "       cnbc_time\n",
      "0    3 Hours Ago\n",
      "1    4 Hours Ago\n",
      "2    4 Hours Ago\n",
      "3    4 Hours Ago\n",
      "4    4 Hours Ago\n",
      "5    5 Hours Ago\n",
      "6    5 Hours Ago\n",
      "7    5 Hours Ago\n",
      "8    6 Hours Ago\n",
      "9    6 Hours Ago\n",
      "10   6 Hours Ago\n",
      "11   6 Hours Ago\n",
      "12  16 Hours Ago\n",
      "13  19 Hours Ago\n",
      "14  19 Hours Ago\n",
      "15  19 Hours Ago\n",
      "16  21 Hours Ago\n",
      "17  21 Hours Ago\n",
      "18  21 Hours Ago\n",
      "19  22 Hours Ago\n",
      "20  22 Hours Ago\n",
      "21  22 Hours Ago\n",
      "22  23 Hours Ago\n",
      "23  23 Hours Ago\n",
      "24  23 Hours Ago\n",
      "25  23 Hours Ago\n",
      "26  23 Hours Ago\n",
      "27  23 Hours Ago\n",
      "28  23 Hours Ago\n",
      "29  24 Hours Ago\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++ \n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++ \n",
      "\n",
      "News Link\n",
      "                                      cnbc_newslink\n",
      "0                                   Skip Navigation\n",
      "1                                                  \n",
      "2                                                  \n",
      "3                                           Markets\n",
      "4                                       Pre-Markets\n",
      "..                                              ...\n",
      "487                                       CA Notice\n",
      "488  New Terms of Service (Updated August 24, 2023)\n",
      "489                      A Division of NBCUniversal\n",
      "490        Market Data Terms of Use and Disclaimers\n",
      "491                                                \n",
      "\n",
      "[492 rows x 1 columns]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame-\n",
    "# i)\tHeadline\n",
    "# ii)\tTime\n",
    "# iii)\tNews Link\n",
    "\n",
    "def headline_cnbc(cnbc_URL):\n",
    "    page = requests.get(cnbc_URL)\n",
    "    print(\"Page Status_code: \", page.status_code)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "    cnbc = []\n",
    "    headline = soup.find_all(\"div\", class_=\"RiverHeadline-headline RiverHeadline-hasThumbnail\")\n",
    "\n",
    "    # Extract the headlines and store them in a list\n",
    "    cnbc = [i.text.strip() for i in headline]\n",
    "    return cnbc\n",
    "\n",
    "def time_cnbc(cnbc_URL):\n",
    "    page = requests.get(cnbc_URL)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "    Time = []\n",
    "    timeline = soup.find_all('time')\n",
    "    Time = [i.text.strip() for i in timeline]\n",
    "    \n",
    "    return Time\n",
    "\n",
    "def newslink_cnbc(cnbc_URL):\n",
    "    page = requests.get(cnbc_URL)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "    \n",
    "    newslink = []\n",
    "    for i in soup.find_all('a'):\n",
    "        newslink.append(i.text)\n",
    "    \n",
    "    return newslink\n",
    "\n",
    "cnbc_URL = \"https://www.cnbc.com/world/?region=world\"\n",
    "cnbc_headlines = headline_cnbc(cnbc_URL)\n",
    "df_cnbc_headerlines = pd.DataFrame({\"cnbc_headlines\": cnbc_headlines})\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++ \\n\")\n",
    "print(\"Headline\")\n",
    "print(df_cnbc_headerlines)\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++ \\n\")\n",
    "\n",
    "cnbc_time = time_cnbc(cnbc_URL)\n",
    "df_cnbc_time = pd.DataFrame({\"cnbc_time\": cnbc_time})\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++ \\n\")\n",
    "print(\"Time\")\n",
    "print(df_cnbc_time)\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++ \\n\")\n",
    "\n",
    "cnbc_newslink = newslink_cnbc(cnbc_URL)\n",
    "df_cnbc_newslink = pd.DataFrame({\"cnbc_newslink\": cnbc_newslink})\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++ \\n\")\n",
    "print(\"News Link\")\n",
    "print(df_cnbc_newslink)\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++ \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14a6b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Status_code:  200\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++ \n",
      "\n",
      "Paper Title,  Author,   Date,   URL  \n",
      "                                        journal_title  \\\n",
      "0                                    Reward is enough   \n",
      "1   Explanation in artificial intelligence: Insigh...   \n",
      "2              Creativity and artificial intelligence   \n",
      "3   Conflict-based search for optimal multi-agent ...   \n",
      "4   Knowledge graphs as tools for explainable mach...   \n",
      "5   Law and logic: A review from an argumentation ...   \n",
      "6   Between MDPs and semi-MDPs: A framework for te...   \n",
      "7   Explaining individual predictions when feature...   \n",
      "8       Multiple object tracking: A literature review   \n",
      "9   A survey of inverse reinforcement learning: Ch...   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   \n",
      "11  Explainable AI tools for legal reasoning about...   \n",
      "12            Hard choices in artificial intelligence   \n",
      "13  Assessing the communication gap between AI mod...   \n",
      "14  Explaining black-box classifiers using post-ho...   \n",
      "15  The Hanabi challenge: A new frontier for AI re...   \n",
      "16              Wrappers for feature subset selection   \n",
      "17  Artificial cognition for social human–robot in...   \n",
      "18  A review of possible effects of cognitive bias...   \n",
      "19  The multifaceted impact of Ada Lovelace in the...   \n",
      "20  Robot ethics: Mapping the issues for a mechani...   \n",
      "21          Reward (Mis)design for autonomous driving   \n",
      "22  Planning and acting in partially observable st...   \n",
      "23  What do we want from Explainable Artificial In...   \n",
      "\n",
      "                                       journal_author    journal_date  \\\n",
      "0   David Silver, Satinder Singh, Doina Precup, Ri...    October 2021   \n",
      "1                                          Tim Miller   February 2019   \n",
      "2                                   Margaret A. Boden     August 1998   \n",
      "3   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...   February 2015   \n",
      "4                      Ilaria Tiddi, Stefan Schlobach    January 2022   \n",
      "5                      Henry Prakken, Giovanni Sartor    October 2015   \n",
      "6     Richard S. Sutton, Doina Precup, Satinder Singh     August 1999   \n",
      "7           Kjersti Aas, Martin Jullum, Anders Løland  September 2021   \n",
      "8                Wenhan Luo, Junliang Xing and 4 more      April 2021   \n",
      "9                       Saurabh Arora, Prashant Doshi     August 2021   \n",
      "10  Jasper van der Waa, Elisabeth Nieuwburg, Anita...   February 2021   \n",
      "11  Joe Collenette, Katie Atkinson, Trevor Bench-C...      April 2023   \n",
      "12   Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz   November 2021   \n",
      "13  Oskar Wysocki, Jessica Katharine Davies and 5 ...      March 2023   \n",
      "14  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...        May 2021   \n",
      "15          Nolan Bard, Jakob N. Foerster and 13 more      March 2020   \n",
      "16                         Ron Kohavi, George H. John   December 1997   \n",
      "17      Séverin Lemaignan, Mathieu Warnier and 3 more       June 2017   \n",
      "18    Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz       June 2021   \n",
      "19                             Luigia Carlucci Aiello       June 2016   \n",
      "20             Patrick Lin, Keith Abney, George Bekey      April 2011   \n",
      "21     W. Bradley Knox, Alessandro Allievi and 3 more      March 2023   \n",
      "22  Leslie Pack Kaelbling, Michael L. Littman, Ant...        May 1998   \n",
      "23             Markus Langer, Daniel Oster and 6 more       July 2021   \n",
      "\n",
      "                                          journal_url  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Write a python program to scrape the details of most downloaded articles from AI in last 90 days.\n",
    "# https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details \n",
    "# and make data frame\n",
    "# i)\tPaper Title\n",
    "# ii)\tAuthors\n",
    "# iii)\tPublished Date\n",
    "# iv)\tPaper URL\n",
    "\n",
    "def title_journal(journal_URL):\n",
    "    page = requests.get(journal_URL)\n",
    "    print(\"Page Status_code: \", page.status_code)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "  \n",
    "    title_ele = soup.find_all(\"h2\")\n",
    "    title = [i.text.strip() for i in title_ele]\n",
    "\n",
    "    return title\n",
    "\n",
    "def author_journal(journal_URL):\n",
    "    page = requests.get(journal_URL)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "  \n",
    "    author_ele = soup.find_all('span',class_=\"sc-1w3fpd7-0 dnCnAO\")\n",
    "    author = [i.text.strip() for i in author_ele]\n",
    "\n",
    "    return author\n",
    "\n",
    "def date_journal(journal_URL):\n",
    "    page = requests.get(journal_URL)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "  \n",
    "    date_ele = soup.find_all('span',class_=\"sc-1thf9ly-2 dvggWt\") \n",
    "    date = [i.text.strip() for i in date_ele]\n",
    "\n",
    "    return date\n",
    "\n",
    "def url_journal(journal_URL):\n",
    "    page = requests.get(journal_URL)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    #print(\"Page Content\", soup)\n",
    "    \n",
    "    article_ele = soup.find_all(\"li\", class_=\"sc-9zxyh7-1 sc-9zxyh7-2 kOEIEO hvoVxs\")\n",
    "\n",
    "    paper_urls = []\n",
    "\n",
    "    for i in article_ele:\n",
    "        urls = i.find(\"a\", class_=\"sc-5smygv-0 fIXTHm\")['href']\n",
    "        paper_urls.append(urls)\n",
    "\n",
    "    return paper_urls\n",
    "\n",
    "journal_URL = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "journal_title = title_journal(journal_URL)\n",
    "journal_author = author_journal(journal_URL)\n",
    "journal_date = date_journal(journal_URL)\n",
    "journal_url = url_journal(journal_URL)\n",
    "df_journal = pd.DataFrame({\"journal_title\": journal_title, \"journal_author\": journal_author, \"journal_date\": journal_date, \"journal_url\": journal_url})\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++ \\n\")\n",
    "print(\"Paper Title,  Author,   Date,   URL  \")\n",
    "print(df_journal)\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++ \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bdb760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
